{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56fe1738-3cd0-4c5b-a430-e6121b00967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be5c26b2-eb94-4a2e-9c15-ee8999b15965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SUJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\SUJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SUJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SUJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sb_stemmer = SnowballStemmer('english')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import words\n",
    "correct_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2c89408-3632-4e40-82bc-e5c06ffa9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68b66dc0-0b00-4690-9039-56607bb56ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[] #tags are the intent of the data in which user resonses are predicted\n",
    "uniquetags=[]\n",
    "patterns=[] #patterns are the possible user responses\n",
    "responses=[] #responses are the random reponse chatbot should give for the predicted intent\n",
    "\n",
    "words =[] #vocabulary of intent classification lemitized in responses\n",
    "for element in data['intents']:\n",
    "    uniquetags.append(element['tag'])    \n",
    "    for patternSentence in element['patterns']:# patterns and the tags are created for the classification performed on later stage\n",
    "        tags.append(element['tag'])\n",
    "        patterns.append(patternSentence)    \n",
    "        \n",
    "    for patternLines in element['patterns']:#patterns are lemmitized and added for better performance\n",
    "         for wordInLine in patternLines.split():\n",
    "            lemm_word=lemmatizer.lemmatize(wordInLine.lower())\n",
    "            if lemm_word not in words:\n",
    "                words.append(lemm_word)                \n",
    "    \n",
    "    responses.append(element['responses'])#responses added from json file to array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c89d20f7-fe8a-40b7-8760-3f97e3d2449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent classification data is splitted into train and test dataset for training the model\n",
    "x_train,x_test,y_train,y_test= train_test_split(patterns,tags,stratify=tags,test_size=0.2,random_state=42)#stratify=tags,\n",
    "count_vect = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "x_train_counts= count_vect.fit_transform(x_train)#x_train\n",
    "\n",
    "#training the model\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True,sublinear_tf=True).fit(x_train_counts)\n",
    "x_train_tf= tfidf_transformer.transform(x_train_counts)\n",
    "x_new_counts = count_vect.transform(x_test)\n",
    "x_new_tfidf = tfidf_transformer.transform(x_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "101492df-05b5-4c50-b8ea-9aa577aca17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "from sklearn import svm\n",
    "svcclf = svm.SVC()\n",
    "svcclf.fit(x_train_tf, y_train)\n",
    "svcpredicted= svcclf.predict(x_new_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0eb1a06f-0c7f-49cb-9a6b-7c624e4fe802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general knowledge section #tokenizing and kemmitizing the sentences   \n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into an array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "#predict the intent for given user input\n",
    "def predict_class(sentence):\n",
    "    doc=[]\n",
    "    doc.append(sentence)\n",
    "    x_new_counts = count_vect.transform(doc) # x_test = arrayOfWords\n",
    "    x_new_tfidf = tfidf_transformer.transform(x_new_counts)\n",
    "    predicted= svcclf.predict(x_new_tfidf)\n",
    "    return predicted\n",
    "\n",
    "#get random response of predicted tag \n",
    "def getResponse(ints,intents_json):\n",
    "    tag=ints\n",
    "    result=\"\"\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag[0]):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "#chatbot work here is to predict the tag and get the response for predicted tag\n",
    "def chatbot_response(text):\n",
    "    ints = predict_class(text)\n",
    "    res = getResponse(ints, data)\n",
    "    return res\n",
    "\n",
    "#get teh name of the person from the sentence #FOR IDENTITY MANAGEMENT FEATURE\n",
    "def getName(sentence):\n",
    "    postArray=\" \"\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    post = pos_tag(sentence.split())\n",
    "    for postArray in post[::-1]:\n",
    "        if(postArray[1]=='NN' or postArray[1]=='NNP' or postArray[1]=='JJ'):\n",
    "            return (postArray[0]).capitalize()\n",
    "\n",
    "#load the dataset for QUESTION ANSWERING FEATURE\n",
    "import pandas as pd;\n",
    "df=pd.read_csv(\"COMP3074-CW1-Dataset.csv\");\n",
    "df.columns=[\"Question_ID\",\"Question\",\"Answer\",\"Document\"];\n",
    "global qa_documents\n",
    "qa_documents={}\n",
    "global topiclist\n",
    "topiclist=[]\n",
    "topic_documents={}\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "        if pd.isnull(df.values).any(1)[index]:\n",
    "            row[\"Answer\"]=\"I dont know. I will ask teacher to let u know.\" #give this response if the data is not available\n",
    "            \n",
    "        if row[\"Question\"] in qa_documents:\n",
    "            qa_documents[row[\"Question\"]]= qa_documents[row[\"Question\"]] + \" \" + row[\"Answer\"]#merge the answers for same question\n",
    "        else:\n",
    "            qa_documents[row[\"Question\"]]= row[\"Answer\"];\n",
    "        \n",
    "        if row[\"Document\"] not in topiclist and row[\"Document\"] is not None:\n",
    "            topiclist.append(row[\"Document\"])#creating topic list\n",
    "            \n",
    "        topic_documents[row[\"Question\"]]=row[\"Document\"]\n",
    "\n",
    "#tokenize sentence and remove stopwords if required\n",
    "def clean_sentence(sentence, stopwordsAbsent=False):\n",
    "    tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    text_tokens = tokenizer.tokenize(sentence)\n",
    "    tokens_without_sw = [word.lower() for word in text_tokens if not word in stopwords.words('english')] #convert to lowercase\n",
    "    if stopwordsAbsent:\n",
    "        sentence = (\" \").join(tokens_without_sw)\n",
    "    return sentence\n",
    "\n",
    "#tokenize each answers\n",
    "def get_cleaned_sentence(df, stopwordsAbsent=True):\n",
    "    sents=df[[\"Question\"]];\n",
    "    cleaned_sentences=[]\n",
    "    for index,row in df.iterrows():\n",
    "        cleaned=clean_sentence(row[\"Question\"],stopwordsAbsent);\n",
    "        cleaned_sentences.append(cleaned);\n",
    "    return cleaned_sentences;\n",
    "\n",
    "#tokenization\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "tok_documents={}\n",
    "for title in qa_documents:\n",
    "    tok_documents[title] = tokenizer.tokenize(title)\n",
    "\n",
    "#remove stopwords and normalise casing\n",
    "english_stopwords = stopwords.words('english')\n",
    "filtered_documents={}\n",
    "for title in tok_documents:\n",
    "    filtered_documents[title] = [word.lower() for word in tok_documents[title] if word.lower() not in english_stopwords]   \n",
    "\n",
    "#do the stemming\n",
    "stemmed_documents ={}\n",
    "for title in filtered_documents:\n",
    "    stemmed_documents[title]= [sb_stemmer.stem(word) for word in filtered_documents[title]]\n",
    "\n",
    "#create the vocabulary\n",
    "vocabulary =[]\n",
    "for title in stemmed_documents:\n",
    "    for stem in stemmed_documents[title]:\n",
    "        if stem not in vocabulary:\n",
    "            vocabulary.append(stem)\n",
    "\n",
    "#create a bag of words\n",
    "def doc2bow(documents,related_vocab):\n",
    "    bow ={}\n",
    "    for title in documents:\n",
    "        bow[title] = np.zeros(len(related_vocab))# list of same size of vocabulary\n",
    "        for stem in documents[title]:\n",
    "            try:\n",
    "                index= related_vocab.index(stem)\n",
    "                bow[title][index] += 1\n",
    "            except ValueError:\n",
    "                continue     \n",
    "    return bow\n",
    "BOW=doc2bow(stemmed_documents,vocabulary)#BOW of each answers in document\n",
    "\n",
    "#cosine similarity between vector documents\n",
    "def response(question_orig):\n",
    "    #word embedding #to do\n",
    "    question= clean_sentence(question_orig,stopwordsAbsent=True)\n",
    "    questionDocument={}\n",
    "    stemmed_question_doc={}\n",
    "    listOfCorrectWords=[]\n",
    "    questionDocument[0]=question.split()\n",
    "    stemmed_question_doc[question]= [sb_stemmer.stem(word) for word in questionDocument[0]]\n",
    "    question_vector=doc2bow(stemmed_question_doc,vocabulary)[question]\n",
    "    getTheAnswer(question,question_vector,BOW,qa_documents,stemmed_documents)\n",
    "\n",
    "#this code is only for testing and creating report content of this project\n",
    "def manhattan(vector_1, vector_2):\n",
    "    diff = abs(vector_1 - vector_2)\n",
    "    return diff.sum()\n",
    "\n",
    "def jaccard(x,y):\n",
    "    x = np.asarray(x, np.bool) # Not necessary, if you keep your data\n",
    "    y = np.asarray(y, np.bool) # in a boolean array already!\n",
    "    return np.double(np.bitwise_and(x, y).sum()) / np.double(np.bitwise_or(x, y).sum())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "#Get the answers as a response from an excel sheet\n",
    "from scipy import spatial\n",
    "def getTheAnswer(question,question_vector, sentence_vector,qa_documents,sentences):\n",
    "    max_sim=-1\n",
    "    index_sim=-1\n",
    "    sim=0.0001\n",
    "    if(np.sum(question_vector)==0):\n",
    "        print(\"Sorry I am unable to find it\")\n",
    "        return\n",
    "    maxSimIndexArray=[]\n",
    "    \n",
    "    for index in sentence_vector.keys():\n",
    "        try:\n",
    "            sim=1-spatial.distance.cosine(question_vector,sentence_vector[index])\n",
    "        except:\n",
    "            continue\n",
    "        if(sim>max_sim):\n",
    "            max_sim=sim\n",
    "            index_sim=index\n",
    "            maxSimIndexArray=[]\n",
    "            maxSimIndexArray.append(index)\n",
    "        elif(sim==max_sim):\n",
    "            maxSimIndexArray.append(index)\n",
    "    \n",
    "    answer=\"\"\n",
    "    if(len(maxSimIndexArray)>1):\n",
    "        print(\"I am sorry but but it is not clear to me. Are u asking anything like the following?(Please select one question from the following):\")\n",
    "        num=1\n",
    "        for index in maxSimIndexArray :\n",
    "            print(num,\". \",index)\n",
    "            num=num+1\n",
    "        print(num+1,\". None of these\")\n",
    "        user_selection = input()\n",
    "        if(user_selection==num+1):\n",
    "            print(\"Sorry I dont have any info related to this in my database.\")\n",
    "            return \n",
    "        if(not (user_selection).isdigit() or int(user_selection)>=num): \n",
    "            print(\"Sorry but I am unable to understand.\")\n",
    "            return\n",
    "        index_sim=maxSimIndexArray[int(user_selection)-1]\n",
    "    \n",
    "    answer= qa_documents[index_sim]    \n",
    "    \n",
    "    if(index_sim==-1):\n",
    "        print(\"Sorry I am unable to find anything related to this.Please try again.\")\n",
    "    else:    \n",
    "        print(answer)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e8d0a5d-cf5d-4ce8-9061-7c8dc266e904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, My name is Max.\n",
      "I am here to help u in your studies. Here are some things we can do together:\n",
      "1. We can discuss any topic from your syllabus.\n",
      "2. If u have any doubts or u wanna revise any question u can ask me.Just let me know that a doubt.\n",
      "3. If u want to test yourself by any random questions. I can help u with that as well.\n",
      "4. You can ask me to book your exam.\n",
      "5. If you are feeling bored or want to relax, we can play a game as well.\n",
      "6. You can share ur problem with me. I will be always there for u to motivate and encourage u.\n",
      "Well, before we start. What is your name?\n",
      "P.S- Let me know at the end if u like me or not.\n",
      " : "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " sam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: Hi Sam How can I help u today?\n",
      "Sam : "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are SLR cameras\n",
      "Sam : "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 461\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28mprint\u001b[39m(question)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28mprint\u001b[39m(name,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 461\u001b[0m user_answer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m()\n\u001b[0;32m    462\u001b[0m answerSim\u001b[38;5;241m=\u001b[39mcheckTheSimilarityOFAnswers(user_answer,question)\n\u001b[0;32m    463\u001b[0m questionSim\u001b[38;5;241m=\u001b[39mcheckTheSimilarityOFQuestion(user_answer,question)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#game section\n",
    "df_game=pd.read_excel(r'HPCharacters.xlsx');\n",
    "game_documents={}\n",
    "for index,row in df_game.iterrows():\n",
    "    cdf=\"\"\n",
    "    for i in range (1,12):\n",
    "        if pd.isnull(df_game.iloc[index][i]):\n",
    "            continue\n",
    "        cdf = cdf + \" \" + str(df_game.iloc[index][i])\n",
    "    game_documents[row[\"Name\"]] = cdf\n",
    "\n",
    "cleaned_details=\"\"    \n",
    "chatBot=\"\"\n",
    "#Chatbot is considering any random character name\n",
    "chatBot=random.choice(list(game_documents.items()))[0]\n",
    "#print(chatBot)\n",
    "\n",
    "#clean the information we are getting for the character\n",
    "def get_clean_details(chatBot,documents):\n",
    "    details=documents[chatBot]\n",
    "    cleaned_details=clean_sentence(details,True)\n",
    "    return cleaned_details\n",
    "\n",
    "cleaned_details= get_clean_details(chatBot,game_documents)\n",
    "#predict_class function will give whether the word is present or not in the text\n",
    "def predict_character(user_response):\n",
    "    cleaned_question=clean_sentence(user_response,True)\n",
    "    yesOrNo= getYesOrNO(cleaned_details,cleaned_question)\n",
    "    return yesOrNo\n",
    "#creating vocabulary\n",
    "game_vocabulary=[]\n",
    "def createVocabulary(cleaned_passage):\n",
    "    #tokenization\n",
    "    tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    tok_documents={}\n",
    "    related_vocabulary=[]\n",
    "    tok_documents = tokenizer.tokenize(cleaned_passage)\n",
    "    for stem in tok_documents:\n",
    "        if (stem not in related_vocabulary):\n",
    "            related_vocabulary.append(stem)\n",
    "    return related_vocabulary\n",
    "\n",
    "game_vocabulary=createVocabulary(cleaned_details)\n",
    "\n",
    "#creating bag of words on the basis of vocabulary\n",
    "def sentence2bow(sentence,relevant_vocabulary):\n",
    "    bow ={}\n",
    "    bowVector = np.zeros(len(relevant_vocabulary))# list of same size of vocabulary\n",
    "    array=[]\n",
    "    array=sentence.split()\n",
    "    for stem in array:\n",
    "        try:\n",
    "            index= relevant_vocabulary.index(stem)\n",
    "            bowVector[index] += 1\n",
    "        except ValueError:\n",
    "            continue     \n",
    "    return bowVector\n",
    "\n",
    "bowOfPassage=[]\n",
    "bowOfPassage=sentence2bow(cleaned_details,game_vocabulary)\n",
    "\n",
    "#get the yes or no answers from chatbot\n",
    "def getYesOrNO(cleaned_details,cleaned_question):\n",
    "    max_sim=-1\n",
    "    index_sim=-1\n",
    "    bow=sentence2bow(cleaned_question,game_vocabulary)\n",
    "    if(not bow.any()):\n",
    "        sim=-1\n",
    "    try:\n",
    "        sim=1-spatial.distance.cosine(bow,bowOfPassage)\n",
    "    except ValueError:\n",
    "        sim=-1\n",
    "    if(sim>max_sim):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#hint function will generate the hint in the blank format (Harry Potter ->> ----- ------)\n",
    "def hint(show):\n",
    "    #print(chatBot)\n",
    "    splitPersonName=chatBot.split()\n",
    "    if(show):\n",
    "        for word in splitPersonName:\n",
    "            for i in range (0,len(word)):\n",
    "                print(\"_\",end =\" \")\n",
    "            print(\" \",end =\" \");\n",
    "        print(\" \")\n",
    "\n",
    "#if user wants to play again then reset will reset all the values\n",
    "def reset():\n",
    "    global chatBot\n",
    "    chatBot=random.choice(list(game_documents.items()))[0]\n",
    "    global game_vocabulary\n",
    "    game_vocabulary=[]\n",
    "    global cleaned_details\n",
    "    cleaned_details= get_clean_details(chatBot,game_documents)\n",
    "    game_vocabulary=createVocabulary(cleaned_details)\n",
    "    global bowOfPassage\n",
    "    bowOfPassage=[]\n",
    "    bowOfPassage=sentence2bow(cleaned_details,game_vocabulary)\n",
    "\n",
    "def gameCall():\n",
    "    gameflag=1\n",
    "    count=0\n",
    "    print(\"--------------------------------------------------------------------------------------\")\n",
    "    print(\"Max:Hi there! Lets check if u can guess the famous characters from 'Harry Potter' series\")\n",
    "    print(\"Rules:\")\n",
    "    print(\"1. You can ask me questions related to that person. e.g. Is he/she male or female? Is she a student? Is he a professor?\")\n",
    "    print(\"2. I will say only Yes or No.\")\n",
    "    print(\"3. You have to guess the person in 10 yes or no questions.\")\n",
    "    print(\"Ok as you are a beginer I will tell you if you are close or very close also...just for you my friend\")\n",
    "    print(\"If you need a hint just say 'Hint'. I will give you one clue. There is only one clue available.\")\n",
    "    #print(\"Your score will be decided on your number of questions asked and on the hint taken it or not.\")\n",
    "    print(\"I hope you are clear with the rules. So lets begin. I have decided one person in my mind. Guess Who?\")\n",
    "    print(\"--------------------------------------------------------------------------------------\")\n",
    "    while gameflag==1:\n",
    "        hint(False)\n",
    "        print(name,\": \",end=\"\")\n",
    "        user_response=input()\n",
    "        user_response = spellCorrector(user_response)\n",
    "        count=count+1;\n",
    "        if((user_response).lower()==\"hint\"): \n",
    "            hint(True)\n",
    "        elif(predict_class(user_response)==\"quit\" or count==10 or user_response.lower==\"i quit\"):\n",
    "            if count==5 : print(\"Max: You lost your 10 chances.\")\n",
    "            print(\"Max: The character is \",chatBot)\n",
    "            #print(game_documents[chatBot])\n",
    "            print(\"Max:Do you wanna play it again?\")\n",
    "            print(name,\": \",end=\"\")\n",
    "            repeat=input()\n",
    "            gameflag=0\n",
    "            if(repeat.lower()=='yes'):\n",
    "                reset()\n",
    "                gameflag=1\n",
    "            else:\n",
    "                print(\"What do you wanna do next?\")\n",
    "        elif((user_response).lower()==(chatBot).lower()):\n",
    "            print(\"Max:Congratulations!!! You guessed it correctly. You are so smart. Good game.\")\n",
    "            print(\"Do you wanna play it again?\")\n",
    "            print(name,\": \",end=\"\")\n",
    "            repeat=input()\n",
    "            gameflag=0\n",
    "            if(repeat.lower()=='yes'):\n",
    "                reset()\n",
    "                gameflag=1\n",
    "        elif(predict_class(user_response)!='bye'):\n",
    "            similarity=predict_character(user_response)\n",
    "            if(similarity):\n",
    "                print(\"Max:Yes. You are close.\")\n",
    "            else:\n",
    "                print(\"Max:No\")\n",
    "        else:\n",
    "            gameflag=0\n",
    "\n",
    "\n",
    "#transaction feature: For booking a test\n",
    "testBooking={\"date\":[\"Ok great. For what date and start time I should book the test? \",\"Thats awesome. On what date and start time u wanna appear for this test? \"],\n",
    "             \"bookdate\":[\"Ok great. I have booked it for u on \",\"Thats awesome. Now u have test on \"],\n",
    "             \n",
    "             \"Topic\":[ \"Ok great. What should be the topics? \",\"Done. Which topics u wanna cover in this test?\"],\n",
    "             \"bookTopic\":[ \"Ok great. I am adding this/these topic/s - \",\"Done. I have added these topics -\"],\n",
    "             \n",
    "             \"Marks\":[ \"What marks u wanna give for each question? \",\"How many marks would u like to give to every question? \"],\n",
    "            \"bookMarks\":[ \"For confirmation I have added marking per question as \",\"Done. Your marks for each question for this test will be \"],\n",
    "     \n",
    "             \"no of questions\":[ \"How many questions u wanna add? \",\"What should be the number of questions to add in this test? \"],\n",
    "             \"book no of questions\":[ \"Done. There will be total  \",\"Nice. This will be a good test with \"],\n",
    "             \n",
    "             \"total time\":[\"For how many mins?\"],\n",
    "             \"book total time\":[\"Great ur test will be of \"],\n",
    "            \n",
    "             \"Wish luck\":[\"All the best\",\"Best of luck\",\"Do well in exam,\"]         \n",
    "            }\n",
    "#get the topics name from the user input topic list\n",
    "def getTopics(cleanData):\n",
    "    topicsInTest=[]\n",
    "    topics_vocabulary=[]\n",
    "    for topic in topiclist:\n",
    "        for words in createVocabulary(topic):\n",
    "            if(words.lower() not in topics_vocabulary):\n",
    "                topics_vocabulary.append(words.lower())\n",
    "    max_sim=0\n",
    "    for topic in topiclist:\n",
    "        bowOfRequestedTopic=sentence2bow(cleanData,topics_vocabulary)\n",
    "        bowOfTopic=sentence2bow(topic.lower(),topics_vocabulary)\n",
    "        if(not bowOfRequestedTopic.any()):\n",
    "            continue\n",
    "        try:\n",
    "            sim=1-spatial.distance.cosine(bowOfRequestedTopic,bowOfTopic)\n",
    "        except ValueError:\n",
    "            continue     \n",
    "        if(sim>max_sim):\n",
    "            topicsInTest.append(topic)\n",
    "    return topicsInTest\n",
    "\n",
    "#print the confirmed data\n",
    "import dateutil\n",
    "def infoFromUserResponse(sentences,intent):\n",
    "    cleanData=clean_sentence(sentences,True)\n",
    "    numberFromData=0\n",
    "    for words in sentences.split(): \n",
    "        try:\n",
    "            if words.isdigit(): \n",
    "                numberFromData=words\n",
    "        except:\n",
    "            print(\"Transaction is closed.\")\n",
    "            continue\n",
    "    if intent == 'bookdate': \n",
    "        try:\n",
    "            d=dateutil.parser.parse(sentences, fuzzy=True)\n",
    "            print(d)\n",
    "        except:\n",
    "            print(\"Transaction is closed.\")\n",
    "    elif intent == 'bookTopic':\n",
    "        topicListForTest=getTopics(cleanData)\n",
    "        for topic in topicListForTest:\n",
    "            print(\" \",topic)\n",
    "    elif intent == 'bookMarks':\n",
    "        print(numberFromData + \" marks\")\n",
    "    elif intent == 'book no of questions':\n",
    "        print(numberFromData + \" questions\")\n",
    "    elif intent == 'book total time':\n",
    "        print(numberFromData + \" mins\")\n",
    "        \n",
    "#store the transactional data of booking a test    \n",
    "def storeData():\n",
    "    for index,key in enumerate(testBooking):\n",
    "        if(index%2 ==0 ): \n",
    "            print(\"Max: \",end=\"\")\n",
    "            print(random.choice(testBooking[key]))\n",
    "            print(name,\": \",end=\"\")\n",
    "            user_response=input()\n",
    "            if(not has_numbers(user_response)):\n",
    "                user_response = spellCorrector(user_response)\n",
    "        else: \n",
    "            print(\"Max: \",end=\"\")\n",
    "            print(random.choice(testBooking[key]),end=\"\")\n",
    "            try:\n",
    "                infoFromUserResponse(user_response,key)\n",
    "            except:\n",
    "                print(\"Somethign is wrong.\")\n",
    "                print(\"Transaction closed.\")\n",
    "                return False\n",
    "        if(key==\"Wish luck\"):\n",
    "            print(\"All the details are confirmed. Your test is booked.\",end=\" \")\n",
    "            print(random.choice(testBooking[key]),name)\n",
    "            print(\"Max: What would you like to do now?\")\n",
    "        if(user_response==\"bye\"):\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "testmins=0\n",
    "noOfQuestions=0\n",
    "topics=[]\n",
    "marks=0\n",
    "testBookingCompleted=False\n",
    "\n",
    "#to ask questions to user\n",
    "def checkTheSimilarityOFAnswers(user_answer,question):\n",
    "    cleanSentences=clean_sentence(user_answer, stopwordsAbsent=True)\n",
    "    cleaned_details= get_clean_details(question,qa_documents)\n",
    "    answer_vocabulary=[]\n",
    "    answer_vocabulary=createVocabulary(cleaned_details)\n",
    "    bowOfPassage=[]\n",
    "    bowOfPassage=sentence2bow(cleaned_details,answer_vocabulary)\n",
    "    \n",
    "    max_sim=-1\n",
    "    index_sim=-1\n",
    "    bow=sentence2bow(cleanSentences,answer_vocabulary)\n",
    "    if(not bow.any()):\n",
    "        return 0\n",
    "    try:\n",
    "        sim=1-spatial.distance.cosine(bow,bowOfPassage)\n",
    "    except:\n",
    "        sim=0   \n",
    "    return sim\n",
    "\n",
    "def checkTheSimilarityOFQuestion(user_answer,question):\n",
    "    cleanSentences=clean_sentence(user_answer, stopwordsAbsent=True)\n",
    "    cleaned_details= clean_sentence(question, stopwordsAbsent=True)\n",
    "    question_vocabulary=[]\n",
    "    question_vocabulary=createVocabulary(cleaned_details)\n",
    "    bowOfPassage=[]\n",
    "    bowOfPassage=sentence2bow(cleaned_details,question_vocabulary)\n",
    "    \n",
    "    max_sim=-1\n",
    "    index_sim=-1\n",
    "    bow=sentence2bow(cleanSentences,question_vocabulary)\n",
    "    if(not bow.any()):\n",
    "        return 0\n",
    "    try:\n",
    "        sim=1-spatial.distance.cosine(bow,bowOfPassage)\n",
    "    except:\n",
    "        sim=0   \n",
    "    return sim\n",
    "\n",
    "#show results\n",
    "askedQuestions={}\n",
    "def showResults(askedQuestions):\n",
    "    if askedQuestions: \n",
    "        print(\"----------------------------------------------------------------------------------\")\n",
    "        print(\"Report:\")\n",
    "        trunkTopicName=[]\n",
    "        trunkQuestion=[]\n",
    "        score=[]\n",
    "        for question in askedQuestions:\n",
    "            trunkTopicName.append((topic_documents[question][:30] + '...') if len(topic_documents[question]) > 30 else topic_documents[question][:30])\n",
    "            trunkQuestion.append((question[:20] + '...') if len(question) > 20 else question[:20])\n",
    "            score.append(askedQuestions[question])\n",
    "            result=pd.DataFrame(list(zip(trunkQuestion, trunkTopicName, score)))\n",
    "        result.columns=['Question', 'Topic','Score']\n",
    "        result.index = range(1,len(trunkQuestion)+1)\n",
    "        print(result)\n",
    "        if np.mean(score)<50:\n",
    "            print(\"----------------------------------------------------------------------------------\")\n",
    "            print(\"Max: Dont worry\",name,\".We will practice and practice makes a person perfect.\" )\n",
    "\n",
    "#checking user response         \n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "#correct the spells excluding nouns\n",
    "def spellCorrector(sentence):\n",
    "    correctedWords=[]\n",
    "    for word in list(sentence.split()):\n",
    "        if nltk.pos_tag([word])[0][1]=='NNP' or nltk.pos_tag([word])[0][1]=='NN':\n",
    "            correctedWords.append(word)\n",
    "            continue\n",
    "        temp = [(nltk.edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
    "        correctWord=sorted(temp, key = lambda val:val[0])[0][1]\n",
    "        correctedWords.append(correctWord)\n",
    "    correctedSentence=(\" \").join(correctedWords)\n",
    "    return correctedSentence\n",
    "\n",
    "#Discuss any topic from the dataset with chatbot\n",
    "#discussion_df=pd.read_csv(\"C://Users//Sanskruti Jajoo//Desktop//CW1//GameDiscussion.csv\");\n",
    "discussion_df=pd.read_excel(r'GameDiscussion.xlsx');\n",
    "discussion_df.columns=[\"Topic\",\"Content\"];\n",
    "global discussion_documents\n",
    "discussion_documents={}\n",
    "\n",
    "for index,row in discussion_df.iterrows():\n",
    "        if pd.isnull(discussion_df.values).any(1)[index]:\n",
    "            row[\"Content\"]=\"I dont know\"\n",
    "            \n",
    "        if row[\"Topic\"] in discussion_documents:\n",
    "            discussion_documents[row[\"Topic\"]]= discussion_documents[row[\"Topic\"]] + \" \" + row[\"Content\"]\n",
    "        else:\n",
    "            discussion_documents[row[\"Topic\"]]= row[\"Content\"];\n",
    "\n",
    "all_topicsOfDiscussion=discussion_documents.keys()\n",
    "\n",
    "def discussion():\n",
    "    print(\"----------------------------------------------------------------------------------------------\")\n",
    "    print(\"Lets have a discussion. I will say something related to that topic and you will also have to say something related to that topic\")\n",
    "    print(\"You can not use the same sentence again and again.\")\n",
    "    print(\"----------------------------------------------------------------------------------------------\")\n",
    "    topic_discussion= random.choice(list(all_topicsOfDiscussion))\n",
    "    print(\"Topic of our discussion is :\",topic_discussion)\n",
    "    \n",
    "    discussion_documents_lines=discussion_documents[topic_discussion].split(\".\")\n",
    "    discussion_vocabulary=createVocabulary(discussion_documents[topic_discussion])\n",
    "    bowOfAllPoints ={}\n",
    "    for index in range(0,len(discussion_documents_lines)):\n",
    "        bowOfAllPoints[index] = np.zeros(len(discussion_vocabulary))# list of same size of vocabulary\n",
    "        for stem in discussion_documents_lines[index].split():\n",
    "            try:\n",
    "                position= discussion_vocabulary.index(stem)\n",
    "                bowOfAllPoints[index][position] += 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "    allLines=[]\n",
    "    allFirstLine=[]\n",
    "    discussion_flag=True\n",
    "    chatbotPoint=random.choice(discussion_documents_lines)\n",
    "    allFirstLine.append(chatbotPoint)\n",
    "    while discussion_flag:\n",
    "        print(\"Max:\",end=\" \")\n",
    "        for line in discussion_documents_lines:\n",
    "            if(allFirstLine==None): continue\n",
    "            for previousline in allLines:\n",
    "                sim= 1/(1+nltk.jaccard_distance(set(line), set(previousline)))\n",
    "                if sim>0.6:\n",
    "                    continue\n",
    "                else:\n",
    "                    chatbotPoint=random.choice(discussion_documents_lines)\n",
    "        print(chatbotPoint)\n",
    "        print(name,\":\",end=\"\")\n",
    "        user_point=input()\n",
    "        allLines.append(chatbotPoint)\n",
    "        if user_point.lower()==\"i quit\":\n",
    "            discussion_flag=False\n",
    "            print(\"Max: What do u wanna do now?\")\n",
    "            continue\n",
    "        bowOFUserPoint=sentence2bow(user_point,discussion_vocabulary)\n",
    "        evaluateTheUserPoint(bowOFUserPoint,bowOfAllPoints,user_point,chatbotPoint,allLines)\n",
    "        allLines.append(user_point)\n",
    "        \n",
    "#check the similairty and give the score and also check whether the users point is repeated or not        \n",
    "def evaluateTheUserPoint(bowOFUserPoint,bowOfAllPoints,user_point,chatbotPoint,allLines):\n",
    "    max_sim=-1\n",
    "    for bowOfPresentSentence in bowOfAllPoints:\n",
    "        score=1-spatial.distance.cosine(bowOFUserPoint,bowOfAllPoints.get(bowOfPresentSentence))\n",
    "        if score>max_sim:\n",
    "            max_sim=score\n",
    "\n",
    "    print(\"Score:\",max_sim)\n",
    "\n",
    "    if max_sim < 0.1 :\n",
    "            print(\"Nice try but next time please make it more relatable. I know you can do it.\")\n",
    "    totalPresent=0\n",
    "    repeated=False\n",
    "    for chatbotPoints in allLines:\n",
    "        totalPresent=0\n",
    "        for words in user_point.split():\n",
    "            if words in chatbotPoints.split():\n",
    "                totalPresent=totalPresent+1\n",
    "        percentagePresent=totalPresent/len(chatbotPoint.split())\n",
    "        if percentagePresent>0.6:\n",
    "            repeated=True\n",
    "    if repeated: print(\"Max:Please dont repeat. Try to remember something else. Comeeeeeee Onnnnnn.\")\n",
    "\n",
    "\n",
    "#code to get user question\n",
    "flag=True\n",
    "print(\"Hi there, My name is Max.\")\n",
    "print(\"I am here to help u in your studies. Here are some things we can do together:\")\n",
    "print(\"1. We can discuss any topic from your syllabus.\")\n",
    "print(\"2. If u have any doubts or u wanna revise any question u can ask me.Just let me know that a doubt.\")\n",
    "print(\"3. If u want to test yourself by any random questions. I can help u with that as well.\")\n",
    "print(\"4. You can ask me to book your exam.\")\n",
    "print(\"5. If you are feeling bored or want to relax, we can play a game as well.\")\n",
    "print(\"6. You can share ur problem with me. I will be always there for u to motivate and encourage u.\")\n",
    "print(\"Well, before we start. What is your name?\")\n",
    "print(\"P.S- Let me know at the end if u like me or not.\")\n",
    "nameFlag=True\n",
    "name=\"\"\n",
    "gkQuestion=False\n",
    "while(flag):\n",
    "    print(name,\": \",end=\"\")\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(predict_class(user_response)!='goodbye'):\n",
    "        if(predict_class(user_response)=='thanks'):\n",
    "            print(\"Max: You are welcome..\",name)\n",
    "        elif(nameFlag):\n",
    "            name=getName(user_response)\n",
    "            if(name==None): name=user_response.capitalize()\n",
    "            print(\"Max: Hi\",name,\"How can I help u today?\")\n",
    "            nameFlag=False\n",
    "        elif(predict_class(user_response)=='discussion'):    \n",
    "            discussion() \n",
    "            gkQuestion=False\n",
    "        elif(predict_class(user_response)=='booking'):\n",
    "            gkQuestion=False\n",
    "            testbooking=storeData()\n",
    "        elif(predict_class(user_response)=='ask'):\n",
    "            gkQuestion=False\n",
    "            question=random.choice(list(qa_documents.items()))[0]\n",
    "            print(question)\n",
    "            print(name,\": \",end=\"\")\n",
    "            user_answer=input()\n",
    "            answerSim=checkTheSimilarityOFAnswers(user_answer,question)\n",
    "            questionSim=checkTheSimilarityOFQuestion(user_answer,question)\n",
    "            if(questionSim>0.5):\n",
    "                print(\"Max: Dont use too much words from question.\")\n",
    "            askedQuestions[question]=answerSim\n",
    "            print(\"Score :\",answerSim*100)\n",
    "            if(answerSim<0.4): \n",
    "                print(\"Max: Dont worry. Try harder next time.\")\n",
    "            else:\n",
    "                print(\"Max: Good job.\")\n",
    "            print(\"Answer: \",qa_documents[question])\n",
    "        elif(predict_class(user_response)=='small talk'):\n",
    "            print(\"Max: Great! lets have a small talk\")\n",
    "            gkQuestion= False\n",
    "        elif(predict_class(user_response)=='game'):\n",
    "            gkQuestion=False\n",
    "            print(\"Max: Lets play the game\")\n",
    "            gameCall()\n",
    "        elif(predict_class(user_response)=='name'):\n",
    "            gkQuestion=False\n",
    "            print(\"Max:\",chatbot_response(user_response),\"My name is MAX and your name is\",name)\n",
    "        elif(predict_class(user_response)=='gk' or predict_class(user_response)=='doubt'):\n",
    "            gkQuestion=True\n",
    "            print(\"Max:Ok\",name,\",I will try my best to help you.Go ahead ask anything.\")\n",
    "        else:\n",
    "            if(not gkQuestion):\n",
    "                print(\"Max: \"+chatbot_response(user_response))\n",
    "            else:\n",
    "                if(not has_numbers(user_response)):\n",
    "                    user_response = spellCorrector(user_response)\n",
    "                print(\"Max: \",end=\"\")\n",
    "                response(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        showResults(askedQuestions)\n",
    "        print(\"Max: Bye\",name,\"!\",chatbot_response(user_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ea5a3-0026-4c0f-b542-c076f52cf3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
